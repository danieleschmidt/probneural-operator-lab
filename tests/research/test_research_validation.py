"""Tests for research validation and benchmarking framework."""

import pytest
import torch
import numpy as np
from pathlib import Path
import tempfile
import shutil
from unittest.mock import Mock, patch

from probneural_operator.benchmarks.research_validation import (
    BenchmarkResult,
    StatisticalTester,
    UncertaintyValidationSuite,
    ResearchBenchmark
)
from probneural_operator.models.fno import ProbabilisticFNO


class TestBenchmarkResult:
    \"\"\"Test BenchmarkResult dataclass.\"\"\"\n    \n    def test_benchmark_result_creation(self):\n        \"\"\"Test basic creation of BenchmarkResult.\"\"\"\n        result = BenchmarkResult(\n            model_name=\"TestFNO\",\n            dataset_name=\"Burgers\",\n            metric_name=\"mse\",\n            value=0.123,\n            std=0.045,\n            ci_lower=0.1,\n            ci_upper=0.15,\n            n_runs=5\n        )\n        \n        assert result.model_name == \"TestFNO\"\n        assert result.dataset_name == \"Burgers\"\n        assert result.metric_name == \"mse\"\n        assert result.value == 0.123\n        assert result.metadata == {}\n    \n    def test_benchmark_result_with_metadata(self):\n        \"\"\"Test BenchmarkResult with custom metadata.\"\"\"\n        metadata = {\"lr\": 0.001, \"epochs\": 100}\n        result = BenchmarkResult(\n            model_name=\"TestFNO\",\n            dataset_name=\"Burgers\",\n            metric_name=\"mse\",\n            value=0.123,\n            std=0.045,\n            ci_lower=0.1,\n            ci_upper=0.15,\n            n_runs=5,\n            metadata=metadata\n        )\n        \n        assert result.metadata == metadata\n\n\nclass TestStatisticalTester:\n    \"\"\"Test statistical significance testing.\"\"\"\n    \n    def test_paired_t_test(self):\n        \"\"\"Test paired t-test functionality.\"\"\"\n        # Create synthetic data where method is better than baseline\n        baseline = np.array([0.8, 0.85, 0.82, 0.88, 0.79])\n        method = np.array([0.7, 0.72, 0.69, 0.75, 0.68])  # Lower is better\n        \n        result = StatisticalTester.paired_t_test(baseline, method, alpha=0.05)\n        \n        assert 't_statistic' in result\n        assert 'p_value' in result\n        assert 'significant' in result\n        assert 'cohens_d' in result\n        assert 'effect_size_magnitude' in result\n        \n        # Should detect improvement (method < baseline)\n        assert result['t_statistic'] > 0  # method better than baseline\n    \n    def test_wilcoxon_signed_rank_test(self):\n        \"\"\"Test Wilcoxon signed-rank test.\"\"\"\n        baseline = np.array([0.8, 0.85, 0.82, 0.88, 0.79])\n        method = np.array([0.7, 0.72, 0.69, 0.75, 0.68])\n        \n        result = StatisticalTester.wilcoxon_signed_rank_test(baseline, method)\n        \n        assert 'wilcoxon_statistic' in result\n        assert 'p_value' in result\n        assert 'significant' in result\n    \n    def test_cohens_d_interpretation(self):\n        \"\"\"Test Cohen's d effect size interpretation.\"\"\"\n        assert StatisticalTester._interpret_cohens_d(0.1) == \"negligible\"\n        assert StatisticalTester._interpret_cohens_d(0.3) == \"small\"\n        assert StatisticalTester._interpret_cohens_d(0.6) == \"medium\"\n        assert StatisticalTester._interpret_cohens_d(1.0) == \"large\"\n        assert StatisticalTester._interpret_cohens_d(-0.6) == \"medium\"  # Absolute value\n\n\nclass TestUncertaintyValidationSuite:\n    \"\"\"Test uncertainty validation methods.\"\"\"\n    \n    def test_validate_calibration(self):\n        \"\"\"Test uncertainty calibration validation.\"\"\"\n        validator = UncertaintyValidationSuite()\n        \n        # Create synthetic well-calibrated data\n        n_samples = 1000\n        true_std = 0.5\n        \n        predictions = torch.randn(n_samples)\n        targets = predictions + torch.randn(n_samples) * true_std\n        uncertainties = torch.ones(n_samples) * true_std\n        \n        metrics = validator.validate_calibration(predictions, uncertainties, targets)\n        \n        assert 'expected_calibration_error' in metrics\n        assert 'average_calibration_error' in metrics\n        assert 'reliability' in metrics\n        assert 'sharpness' in metrics\n        assert 'coverage_correlation' in metrics\n        \n        # Well-calibrated data should have low calibration error\n        assert metrics['expected_calibration_error'] < 0.2\n    \n    def test_validate_calibration_poorly_calibrated(self):\n        \"\"\"Test calibration validation with poorly calibrated uncertainties.\"\"\"\n        validator = UncertaintyValidationSuite()\n        \n        n_samples = 1000\n        \n        predictions = torch.randn(n_samples)\n        targets = predictions + torch.randn(n_samples) * 0.5\n        # Overconfident uncertainties (too small)\n        uncertainties = torch.ones(n_samples) * 0.1\n        \n        metrics = validator.validate_calibration(predictions, uncertainties, targets)\n        \n        # Should detect poor calibration\n        assert metrics['expected_calibration_error'] > 0.1\n    \n    @pytest.mark.skipif(not torch.cuda.is_available(), reason=\"CUDA not available\")\n    def test_validate_uncertainty_decomposition(self):\n        \"\"\"Test uncertainty decomposition validation.\"\"\"\n        validator = UncertaintyValidationSuite()\n        \n        # Create mock probabilistic model\n        model = Mock()\n        model.eval.return_value = None\n        \n        # Mock sample_predictions method\n        n_samples = 50\n        batch_size = 32\n        output_dim = 1\n        spatial_size = 64\n        \n        # Create realistic sample predictions with epistemic and aleatoric components\n        epistemic_std = 0.3\n        aleatoric_std = 0.2\n        \n        def mock_sample_predictions(data, num_samples):\n            batch_size = data.shape[0]\n            # Epistemic uncertainty: variance between different models\n            model_means = torch.randn(num_samples, batch_size, output_dim, spatial_size) * epistemic_std\n            # Aleatoric uncertainty: noise within each model\n            samples = model_means + torch.randn_like(model_means) * aleatoric_std\n            return samples\n        \n        model.sample_predictions = mock_sample_predictions\n        \n        # Create test data loader\n        from torch.utils.data import DataLoader, TensorDataset\n        test_data = torch.randn(batch_size, 1, spatial_size)\n        test_targets = torch.randn(batch_size, output_dim, spatial_size)\n        test_loader = DataLoader(TensorDataset(test_data, test_targets), batch_size=batch_size)\n        \n        metrics = validator.validate_uncertainty_decomposition(model, test_loader)\n        \n        assert 'mean_total_uncertainty' in metrics\n        assert 'mean_epistemic_uncertainty' in metrics\n        assert 'mean_aleatoric_uncertainty' in metrics\n        assert 'decomposition_error' in metrics\n        assert 'epistemic_fraction' in metrics\n        \n        # Verify reasonable uncertainty decomposition\n        assert metrics['mean_total_uncertainty'] > 0\n        assert metrics['mean_epistemic_uncertainty'] > 0\n        assert metrics['mean_aleatoric_uncertainty'] > 0\n        assert 0 <= metrics['epistemic_fraction'] <= 1\n\n\nclass TestResearchBenchmark:\n    \"\"\"Test comprehensive research benchmark framework.\"\"\"\n    \n    def setup_method(self):\n        \"\"\"Set up test environment.\"\"\"\n        self.temp_dir = tempfile.mkdtemp()\n        self.benchmark = ResearchBenchmark(\n            output_dir=self.temp_dir,\n            n_runs=3,  # Reduced for testing\n            random_seed=42\n        )\n    \n    def teardown_method(self):\n        \"\"\"Clean up test environment.\"\"\"\n        shutil.rmtree(self.temp_dir)\n    \n    def test_benchmark_initialization(self):\n        \"\"\"Test benchmark initialization.\"\"\"\n        assert self.benchmark.n_runs == 3\n        assert self.benchmark.confidence_level == 0.95\n        assert self.benchmark.random_seed == 42\n        assert Path(self.temp_dir).exists()\n    \n    def create_mock_data_loaders(self, batch_size=16, n_batches=5):\n        \"\"\"Create mock data loaders for testing.\"\"\"\n        from torch.utils.data import DataLoader, TensorDataset\n        \n        # Create synthetic data\n        input_dim = 32\n        output_dim = 32\n        \n        def create_loader():\n            data = torch.randn(batch_size * n_batches, 1, input_dim)\n            targets = torch.randn(batch_size * n_batches, 1, output_dim)\n            return DataLoader(TensorDataset(data, targets), batch_size=batch_size)\n        \n        return create_loader(), create_loader(), create_loader()\n    \n    def create_mock_model_factory(self):\n        \"\"\"Create mock model factory for testing.\"\"\"\n        def model_factory():\n            # Create a simple mock model\n            model = Mock()\n            model.fit = Mock(return_value={'train_loss': [0.1, 0.05], 'val_loss': [0.2, 0.1]})\n            model.predict_with_uncertainty = Mock(return_value=(\n                torch.randn(16, 1, 32),  # predictions\n                torch.ones(16, 1, 32) * 0.1  # uncertainties\n            ))\n            model.eval = Mock()\n            \n            # Mock forward method\n            def forward(x):\n                return torch.randn(x.shape[0], 1, 32)\n            model.side_effect = forward\n            \n            return model\n        \n        return model_factory\n    \n    def test_benchmark_model(self):\n        \"\"\"Test model benchmarking functionality.\"\"\"\n        train_loader, val_loader, test_loader = self.create_mock_data_loaders()\n        model_factory = self.create_mock_model_factory()\n        \n        training_config = {'epochs': 10, 'lr': 0.001}\n        \n        results = self.benchmark.benchmark_model(\n            model_factory=model_factory,\n            model_name=\"MockFNO\",\n            train_loader=train_loader,\n            val_loader=val_loader,\n            test_loader=test_loader,\n            dataset_name=\"MockDataset\",\n            training_config=training_config,\n            metrics=['mse', 'mae']\n        )\n        \n        assert len(results) == 2  # Two metrics\n        assert all(isinstance(r, BenchmarkResult) for r in results)\n        assert all(r.model_name == \"MockFNO\" for r in results)\n        assert all(r.dataset_name == \"MockDataset\" for r in results)\n        assert {r.metric_name for r in results} == {'mse', 'mae'}\n    \n    def test_compare_methods(self):\n        \"\"\"Test statistical comparison between methods.\"\"\"\n        # Create mock results for comparison\n        baseline_results = [\n            BenchmarkResult(\n                model_name=\"Baseline\",\n                dataset_name=\"Test\",\n                metric_name=\"mse\",\n                value=0.8,\n                std=0.1,\n                ci_lower=0.7,\n                ci_upper=0.9,\n                n_runs=3\n            )\n        ]\n        \n        method_results = [\n            BenchmarkResult(\n                model_name=\"NewMethod\",\n                dataset_name=\"Test\",\n                metric_name=\"mse\",\n                value=0.6,\n                std=0.08,\n                ci_lower=0.55,\n                ci_upper=0.65,\n                n_runs=3\n            )\n        ]\n        \n        comparison = self.benchmark.compare_methods(\n            baseline_results, method_results, metric=\"mse\"\n        )\n        \n        assert 'metric' in comparison\n        assert 'baseline_mean' in comparison\n        assert 'method_mean' in comparison\n        assert 'improvement_percent' in comparison\n        assert 't_test' in comparison\n        assert 'wilcoxon_test' in comparison\n        assert 'recommendation' in comparison\n        \n        # Should detect improvement (method < baseline for MSE)\n        assert comparison['improvement_percent'] > 0\n    \n    def test_generate_report(self):\n        \"\"\"Test report generation.\"\"\"\n        # Add some mock results\n        result1 = BenchmarkResult(\n            model_name=\"FNO\",\n            dataset_name=\"Burgers\",\n            metric_name=\"mse\",\n            value=0.123,\n            std=0.045,\n            ci_lower=0.1,\n            ci_upper=0.15,\n            n_runs=3\n        )\n        \n        result2 = BenchmarkResult(\n            model_name=\"FNO\",\n            dataset_name=\"Burgers\",\n            metric_name=\"mae\",\n            value=0.089,\n            std=0.023,\n            ci_lower=0.08,\n            ci_upper=0.098,\n            n_runs=3\n        )\n        \n        self.benchmark.results = [result1, result2]\n        \n        # Generate report (without plots to avoid matplotlib issues in tests)\n        report_path = self.benchmark.generate_report(\n            include_plots=False,\n            include_tables=True\n        )\n        \n        assert Path(report_path).exists()\n        assert Path(report_path).suffix == '.md'\n        \n        # Check that various files were created\n        report_dir = Path(report_path).parent\n        assert (report_dir / \"summary_table.csv\").exists()\n        assert (report_dir / \"raw_results.json\").exists()\n    \n    def test_empty_results_handling(self):\n        \"\"\"Test handling of empty or failed benchmark results.\"\"\"\n        # Test comparison with empty results\n        with pytest.raises(ValueError, match=\"No results found for metric\"):\n            self.benchmark.compare_methods([], [], metric=\"mse\")\n    \n    def test_interpretation_of_comparison_results(self):\n        \"\"\"Test interpretation of statistical comparison results.\"\"\"\n        # Strong evidence of improvement\n        t_test_sig = {'significant': True, 'effect_size_magnitude': 'large'}\n        wilcoxon_sig = {'significant': True}\n        interpretation = self.benchmark._interpret_comparison_results(\n            t_test_sig, wilcoxon_sig, improvement=15.0\n        )\n        assert \"Strong evidence of improvement\" in interpretation\n        \n        # No significant difference\n        t_test_nonsig = {'significant': False}\n        wilcoxon_nonsig = {'significant': False}\n        interpretation = self.benchmark._interpret_comparison_results(\n            t_test_nonsig, wilcoxon_nonsig, improvement=2.0\n        )\n        assert \"No significant difference\" in interpretation\n\n\n@pytest.mark.integration\nclass TestResearchBenchmarkIntegration:\n    \"\"\"Integration tests for research benchmark with real models.\"\"\"\n    \n    def setup_method(self):\n        \"\"\"Set up integration test environment.\"\"\"\n        self.temp_dir = tempfile.mkdtemp()\n    \n    def teardown_method(self):\n        \"\"\"Clean up integration test environment.\"\"\"\n        shutil.rmtree(self.temp_dir)\n    \n    @pytest.mark.slow\n    @pytest.mark.skipif(not torch.cuda.is_available(), reason=\"CUDA required for integration test\")\n    def test_full_benchmark_with_real_model(self):\n        \"\"\"Test full benchmarking pipeline with real ProbabilisticFNO.\"\"\"\n        benchmark = ResearchBenchmark(\n            output_dir=self.temp_dir,\n            n_runs=2,  # Reduced for CI\n            random_seed=42\n        )\n        \n        # Create real data loaders with small datasets\n        from torch.utils.data import DataLoader, TensorDataset\n        \n        batch_size = 8\n        n_samples = 32\n        spatial_size = 16\n        \n        def create_loader():\n            data = torch.randn(n_samples, 1, spatial_size)\n            targets = torch.randn(n_samples, 1, spatial_size)\n            return DataLoader(TensorDataset(data, targets), batch_size=batch_size)\n        \n        train_loader, val_loader, test_loader = create_loader(), create_loader(), create_loader()\n        \n        # Model factory for ProbabilisticFNO\n        def model_factory():\n            return ProbabilisticFNO(\n                input_dim=1,\n                output_dim=1,\n                modes=4,\n                width=16,\n                depth=2,\n                spatial_dim=1,\n                posterior_type=\"laplace\"\n            )\n        \n        training_config = {\n            'epochs': 5,  # Reduced for testing\n            'lr': 0.001,\n            'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n        }\n        \n        # Run benchmark\n        results = benchmark.benchmark_model(\n            model_factory=model_factory,\n            model_name=\"ProbabilisticFNO\",\n            train_loader=train_loader,\n            val_loader=val_loader,\n            test_loader=test_loader,\n            dataset_name=\"SyntheticBurgers\",\n            training_config=training_config,\n            metrics=['mse', 'mae']\n        )\n        \n        assert len(results) >= 2\n        assert all(isinstance(r, BenchmarkResult) for r in results)\n        assert all(r.n_runs <= 2 for r in results)\n        assert all(r.runtime > 0 for r in results)\n        \n        # Generate report\n        report_path = benchmark.generate_report(include_plots=False)\n        assert Path(report_path).exists()